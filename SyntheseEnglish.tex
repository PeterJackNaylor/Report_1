\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
%%%\geometry{hmargin=2.5cm,vmargin=3cm}

\title{Summary}
\author{Peter Naylor}
\date{08 October 2015}

\begin{document}
\begin{center}
\begin{Large}
\underline{Cell cycle analysis by live imaging}\\
\vspace{0.3cm}
\textit{Summary}\\
Naylor Peter
\end{Large}
\end{center}

\vspace{3cm} I have studied machine learning algorithms during my final year in ENSAE and wished to apply them to research in biology. Thomas Walter kindly gave me this opportunity by offering this research internship at the centre of computational biology (cbio). Working with another PhD student I was assigned a question, is it possible to predict cell cycle phases based only on a certain fluorescent marker, this marker however is meant to be uninformative for this type of prediction by biologists.

\bigskip

The raw data are films acquired by microscopy and contain cell cycle footage. The cycle goes though different mitotic phases and 3 non-mitotic phases, \textit{G1, S} and \textit{G2}. Our goal is be able separate each phase, in other words different we wish to find a set of rules based on image features that can differentiate two phases from each other. These features can be of very different nature, shape, intensity and some more difficult measures such as haralic features. We have 239 features and a set of 1272 labelled instances. The features and labels were generated by annotating and feature extraction program called \textit{CellCognition}. \textit{CellCognition} is software capable of segmenting cells on a picture; from this segmentation we extract 239 features. Biologist contributed by labelling some the segmented cells. This tool was co-developed by Thomas Walter. Alice Schoenauer Sebag, a PhD student at cbio, had clustered many of the instances by trajectories. Before this clustering, features of a certain cell were extracted into a data set with no relationship between them. Alice created them by tracking cells and studying cell mobility through the entire film.

\bigskip

The final predictive model can be decomposed in three separate entities. The first based on the raw extraction of features will predict if the cell is or is not in a mitotic phase. We needed this “pre-classifier” by lack of mitotic samples in our training set. It is known that this classifier is efficient to detect mitotic phases; the first prediction will take over the second if it predicts mitosis.  The second classifier focuses on the interesting phases, \textit{G1, S} and \textit{G2}. To improve prediction rate, we try to erase all cell-specific attributes, such as initial size, by normalizing the data so that it represents the evolution compared to its initial state. This second classifier is followed by a correcting model that will take into account time dependencies created by Alice's cell tracking. This model yielded 86\% accuracy rate. We notice that the two first phases seem differentiable whereas our classifier seems to have difficulties in finding differences between \textit{S} and \textit{G2}. But it can seem normal if you have a look at the raw imaging, it is hard to distinguish the different phases with your bare eyes.

\bigskip

This training had as an aim the prediction of the MitoCheck data set, a European project of less-of-function experiences of cell live imaging. In other words, we have 200 000 videos of cell imaging and for each film, a certain protein is strongly reduce. This procedure enables us to infer protein function within the cell cycle. If we would be able to track, and correctly predict the phases of the cells for the non-mitotic phase, we could maybe infer information about the role of each protein. This data set has a \texttt{H2B} marker that is usually used for mitotic phase tracking, it is not meant to be informative for the non-mitotic phases. The only issue is that we have no ground truth about the MitoCheck data set. We try to assess the quality of our predictors by studying the lengths of the different non-mitotic phases. For most of the trajectories we only predict one class \textit{G1}. However, the 10\% that have a standard predictive cell cycle have literature-like values for cell cycle phase \textit{G1} only. Moreover, we use transfer learning technics in order to improve accepted trajectory rate. Transfer learning is a branch of machine learning that relaxes one of the key assumptions; the distribution of the training set may differ from the distribution of the predictive set. The main idea is to do a selection of the training samples in order to only train on the instances that represent in some way the predictive set. The result of the transfer learning is a vector of weight corresponding to each different sample, we weight them correspondingly to their importance with respect to the predictive set. Two approaches were tested, both deriving from the same technic: kernel mean matching. In the first, we start with an empty training set and select the most important samples. In the second, we start with the whole training set and down weight the least important variables. The second improves considerably the number of accepted trajectories. The down weight might have had some positive effect whereas the first selected too few instances for training resulting in training over less than 50 different instances. The transfer learning increased the number of accepted trajectories by 70.

\bigskip 

Our classifier seems to be well adapted for the prediction of \textit{G1} but possesses strong priors, based on biological facts. The classifier would be better suited for our purpose if we could relax the priors on the phase transitions. This would prevent the hidden Markov model to force a transition phase if a cell had an unexpected stop in phase \textit{S}, moreover, cells stopping in phases due to the lack of a protein is what we are interested in. Having some ground truth might be vital in order to assess how good our predictions are, it will be provided by EMBL, Heidelberg. Future work on this project implies checking for differences between the distributions of each feature in both respective sets, it may be difficult with the high number of features and may need some dimensionality reduction. We applied instance selection but it might be more interesting to apply feature transformation which is another type of transfer learning.

\end{document}
